{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "98aa9ca3-abb1-4681-8aaa-a078d9c7d386",
      "cell_type": "markdown",
      "source": "# Creating an AI Opponent in Connect4 with Reinforcement Learning",
      "metadata": {
        "id": "98aa9ca3-abb1-4681-8aaa-a078d9c7d386"
      }
    },
    {
      "id": "Iwf-c5QXK1f0",
      "cell_type": "markdown",
      "source": "This notebook explores Reinforcement Learning techniques that can be used to train an AI to play Connect4.",
      "metadata": {
        "id": "Iwf-c5QXK1f0"
      }
    },
    {
      "id": "n5gmRx0BLCOc",
      "cell_type": "markdown",
      "source": "First, let's import the packages necessary to complete this task.",
      "metadata": {
        "id": "n5gmRx0BLCOc"
      }
    },
    {
      "id": "c45e6df5-6b33-4e68-acfa-8aecaeacd71d",
      "cell_type": "code",
      "source": "#Import packages\nimport pygame\nimport sys\nimport numpy as np\nimport random\nimport os",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c45e6df5-6b33-4e68-acfa-8aecaeacd71d",
        "outputId": "52b6b735-a736-4701-c53b-d4c5e4e537f8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "pygame-ce 2.4.1 (SDL 2.28.4, Python 3.12.7)\n"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "u2Qdts79LuiO",
      "cell_type": "markdown",
      "source": "First, we'll create the constant values for the game to be ran.",
      "metadata": {
        "id": "u2Qdts79LuiO"
      }
    },
    {
      "id": "7e833be2-b930-4549-a6d7-23b2c64a239a",
      "cell_type": "code",
      "source": "#Create constants for the game\nROW_COUNT = 6\nCOLUMN_COUNT = 7\nPLAYER1 = 1\nPLAYER2 = 2\nEMPTY = 0\n\n#Pygame display constants\nSQUARESIZE = 100\nRADIUS = int(SQUARESIZE / 2 - 5)\nwidth = COLUMN_COUNT * SQUARESIZE\nheight = (ROW_COUNT + 1) * SQUARESIZE\nsize = (width, height)\n\nBLUE = (0, 0, 255)\nBLACK = (0, 0, 0)\nRED = (255, 0, 0)\nYELLOW = (255, 255, 0)",
      "metadata": {
        "id": "7e833be2-b930-4549-a6d7-23b2c64a239a",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "5G0yNBfaMP8F",
      "cell_type": "markdown",
      "source": "Then we'll make all of the functions for the PyGame to work.",
      "metadata": {
        "id": "5G0yNBfaMP8F"
      }
    },
    {
      "id": "c309a397-bad1-4ffd-bd29-023734b2d282",
      "cell_type": "code",
      "source": "#Pygame Functions\ndef draw_board(board, screen):\n    for c in range(COLUMN_COUNT):\n        for r in range(ROW_COUNT):\n            pygame.draw.rect(screen, BLUE, (c*SQUARESIZE, (r+1)*SQUARESIZE, SQUARESIZE, SQUARESIZE))\n            pygame.draw.circle(screen, BLACK, (int(c*SQUARESIZE + SQUARESIZE/2), int((r+1)*SQUARESIZE + SQUARESIZE/2)), RADIUS)\n\n    for c in range(COLUMN_COUNT):\n        for r in range(ROW_COUNT):\n            if board[r][c] == PLAYER1:\n                pygame.draw.circle(screen, RED, (int(c*SQUARESIZE + SQUARESIZE/2), height - int((r+1)*SQUARESIZE - SQUARESIZE/2)), RADIUS)\n            elif board[r][c] == PLAYER2:\n                pygame.draw.circle(screen, YELLOW, (int(c*SQUARESIZE + SQUARESIZE/2), height - int((r+1)*SQUARESIZE - SQUARESIZE/2)), RADIUS)\n    pygame.display.update()\n\ndef play_gui(agent, env):\n    pygame.init()\n    screen = pygame.display.set_mode(size)\n    font = pygame.font.SysFont(\"monospace\", 75)\n    pygame.display.set_caption(\"Connect 4 - Play vs AI\")\n\n    state = env.reset()\n    draw_board(state, screen)\n    pygame.display.update()\n    turn = PLAYER1\n    game_over = False\n\n    while not game_over:\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                sys.exit()\n\n            #Have coin follow mouse\n            if event.type == pygame.MOUSEMOTION:\n                pygame.draw.rect(screen, BLACK, (0,0, width, SQUARESIZE))\n                posx = event.pos[0]\n                if turn == PLAYER1:\n                    pygame.draw.circle(screen, RED, (posx, int(SQUARESIZE/2)), RADIUS)\n                pygame.display.update()\n\n            #Put coin where clicked\n            if turn == PLAYER1 and event.type == pygame.MOUSEBUTTONDOWN:\n                x_pos = event.pos[0]\n                col = int(x_pos / SQUARESIZE)\n\n                if col in env.valid_actions():\n                    state, reward, done = env.step(col, PLAYER1)\n                    draw_board(state, screen)\n\n                    if done:\n                        if env.winner == PLAYER1:\n                            label = font.render(\"You win!\", 1, RED)\n                        else:\n                            label = font.render(\"Draw!\", 1, RED)\n                        screen.blit(label, (40, 10))\n                        pygame.display.update()\n                        pygame.time.wait(3000)\n                        game_over = True\n                        break\n\n                    turn = PLAYER2\n\n        if turn == PLAYER2 and not game_over:\n            pygame.time.wait(500)  # Delay for realism\n            col = agent.choose_action(env)\n            state, reward, done = env.step(col, PLAYER2)\n            draw_board(state, screen)\n\n            if done:\n                if env.winner == PLAYER2:\n                    label = font.render(\"AI wins!\", 1, YELLOW)\n                else:\n                    label = font.render(\"Draw!\", 1, YELLOW)\n                screen.blit(label, (40, 10))\n                pygame.display.update()\n                pygame.time.wait(3000)\n                game_over = True\n\n            turn = PLAYER1\n\n    pygame.quit()",
      "metadata": {
        "id": "c309a397-bad1-4ffd-bd29-023734b2d282",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "id": "rrMvIuuIKqgM",
      "cell_type": "markdown",
      "source": "## The Environment\n\nThe environment for this project is the classic game of Connect 4.\n\n- **State:** The state of the environment is represented by a 2D NumPy array (6 rows, 7 columns) where each element indicates whether a cell is empty (0), occupied by Player 1 (1), or occupied by Player 2 (2).\n- **Actions:** The available actions are the columns in which a player can drop their piece. A column is a valid action if the topmost cell in that column is empty.\n- **Reward:** The reward is 1 if the agent wins and 0 for a draw.\n- **Completed:** The episode is done when a player wins or when the board is full (a draw).",
      "metadata": {
        "id": "rrMvIuuIKqgM"
      }
    },
    {
      "id": "45e7c9b8-a9e6-4e08-9fc7-e5b144678303",
      "cell_type": "code",
      "source": "#Connect4 Environment\nclass Connect4Env:\n    def __init__(self):\n        self.board = np.zeros((ROW_COUNT, COLUMN_COUNT), dtype=int)\n        self.done = False\n        self.winner = None\n\n    def reset(self):\n        self.board[:] = 0\n        self.done = False\n        self.winner = None\n        return self.get_state()\n\n    def get_state(self):\n        return self.board.copy()\n\n    def valid_actions(self):\n        return [c for c in range(COLUMN_COUNT) if self.board[ROW_COUNT-1][c] == EMPTY]\n\n    def drop_piece(self, col, piece):\n        for r in range(ROW_COUNT):\n            if self.board[r][col] == EMPTY:\n                self.board[r][col] = piece\n                return r\n\n    def winning_move(self, piece):\n        for c in range(COLUMN_COUNT - 3):\n            for r in range(ROW_COUNT):\n                if np.all(self.board[r, c:c+4] == piece):\n                    return True\n        for c in range(COLUMN_COUNT):\n            for r in range(ROW_COUNT - 3):\n                if np.all(self.board[r:r+4, c] == piece):\n                    return True\n        for c in range(COLUMN_COUNT - 3):\n            for r in range(ROW_COUNT - 3):\n                if all(self.board[r+i][c+i] == piece for i in range(4)):\n                    return True\n        for c in range(COLUMN_COUNT - 3):\n            for r in range(3, ROW_COUNT):\n                if all(self.board[r-i][c+i] == piece for i in range(4)):\n                    return True\n        return False\n\n    def step(self, action, piece):\n        if action not in self.valid_actions():\n            return self.get_state(), -10, True\n        self.drop_piece(action, piece)\n\n        reward = 0\n        if self.winning_move(piece):\n            self.done = True\n            self.winner = piece\n            reward = 1\n        elif len(self.valid_actions()) == 0:\n            self.done = True\n            self.winner = 0\n            reward = 0\n        else:\n            reward = 0\n\n        return self.get_state(), reward, self.done",
      "metadata": {
        "id": "45e7c9b8-a9e6-4e08-9fc7-e5b144678303",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "m1shSazkMhu1",
      "cell_type": "markdown",
      "source": "## The Agent\n\nThe agent implemented is a TD(lambda) agent, a type of reinforcement learning agent that uses temporal difference learning with eligibility traces.\n\n- **TD(lambda):** This method updates the agent's value function based on the difference between the predicted value of a state and the actual outcome (reward plus discounted value of the next state), considering a trace of recent states visited. This helps to attribute credit for the outcome to earlier states that contributed to it.\n- **Player:** The agent is initialized with a player number (PLAYER1 or PLAYER2) to distinguish its pieces from the opponent's.\n- **Weights (w):** The agent learns a set of weights that represent the importance of different features in the state for determining its value. These weights are updated during training.\n- **Eligibility Traces (e):** These traces keep track of which features were active and how recently they were active, allowing for credit assignment over multiple time steps.",
      "metadata": {
        "id": "m1shSazkMhu1"
      }
    },
    {
      "id": "_PQVci1_NUIm",
      "cell_type": "markdown",
      "source": "## The Policy\n\nThe agent uses a combination of strategies for choosing an action:\n\n- **Epsilon-Greedy (during training):** The agent explores the environment by choosing a random valid action with a probability of epsilon. Otherwise, it chooses the action that is expected to lead to the state with the highest estimated value. Epsilon decays over training episodes to shift from exploration to exploitation.\n- **Prioritized Actions (during play):** The agent first checks for immediate winning moves and blocks the opponent's winning moves. It also checks for and blocks \"double-sided two-in-a-row\" threats. If none of these immediate threats or opportunities exist, it selects the action that leads to the highest predicted state value.\n- **Middle Column Preference:** Among actions that yield the same highest predicted value, the agent prioritizes dropping a piece in the middle column, which is a common strategy in Connect 4.\n\nThe update method used is the TD(lambda) update rule, which modifies the weights based on the temporal difference error and the eligibility traces.",
      "metadata": {
        "id": "_PQVci1_NUIm"
      }
    },
    {
      "id": "6a0f0455-af3b-4799-aec3-5b80a5443525",
      "cell_type": "code",
      "source": "#Agent Class\nclass TDLambdaAgent:\n    def __init__(self, player, alpha=0.03, gamma=0.9, lam=0.8, epsilon=1.0):\n        self.player = player\n        self.alpha = alpha\n        self.gamma = gamma\n        self.lam = lam\n        self.epsilon = epsilon\n        self.w = np.zeros(87)\n        self.e = np.zeros_like(self.w)\n\n    def set_epsilon(self, eps):\n        self.epsilon = eps\n\n    def save_weights(self, path):\n        np.save(path, self.w)\n\n    def load_weights(self, path):\n        self.w = np.load(path)\n\n    def value(self, state):\n        x = enhanced_features(state, self.player)\n        return np.dot(self.w, x), x\n\n    def choose_action(self, env):\n        valid = env.valid_actions()\n        MIDDLE_COL = 3\n        opponent = PLAYER1 if self.player == PLAYER2 else PLAYER2\n\n        #Check if AI can win immediately\n        for col in valid:\n            temp_env = Connect4Env()\n            temp_env.board = env.board.copy()\n            temp_env.drop_piece(col, self.player)\n            if temp_env.winning_move(self.player):\n                return col  # play winning move\n\n        #Check for immediate block against opponent's 3-in-a-row (winning threat)\n        for col in valid:\n            temp_env = Connect4Env()\n            temp_env.board = env.board.copy()\n            temp_env.drop_piece(col, opponent)\n            if temp_env.winning_move(opponent):\n                return col  # block winning move\n\n        #Check for \"double-sided two-in-a-row\" threats from opponent\n        for col in valid:\n            row = next((r for r in range(ROW_COUNT) if env.board[r][col] == EMPTY), None)\n            if row is not None:\n                temp_board = env.board.copy()\n                temp_board[row][col] = opponent\n                if creates_double_sided_two(temp_board, row, col, opponent):\n                    return col\n\n        #Prefer middle if it's among max-valued actions\n        values = []\n        for a in valid:\n            temp_env = Connect4Env()\n            temp_env.board = env.board.copy()\n            temp_env.done = env.done\n            temp_env.winner = env.winner\n            temp_env.drop_piece(a, self.player)\n            v, _ = self.value(temp_env.board)\n            values.append(v)\n\n        max_val = max(values)\n        max_actions = [a for a, v in zip(valid, values) if v == max_val]\n\n        #Prioritize middle column if it's one of the best options\n        if MIDDLE_COL in max_actions:\n            return MIDDLE_COL\n\n        return random.choice(max_actions)\n\n    def update(self, state, reward, next_state, done):\n        v_s, x_s = self.value(state)\n        v_s_next, x_s_next = (0, np.zeros_like(self.w)) if done else self.value(next_state)\n        delta = reward + self.gamma * v_s_next - v_s\n        self.e = self.gamma * self.lam * self.e + x_s\n        self.w += self.alpha * delta * self.e",
      "metadata": {
        "id": "6a0f0455-af3b-4799-aec3-5b80a5443525",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "id": "J10ZGQbpNs5f",
      "cell_type": "markdown",
      "source": "## Value Function\n\nThe agent uses a linear value function approximator:\n\n$V(s) = w^T \\phi(s)$\n\nwhere:\n- $V(s)$ is the estimated value of state $s$.\n- $w$ is the vector of learned weights.\n- $\\phi(s)$ is the feature vector extracted from state $s$.\n\nThe `enhanced_features` function extracts several features from the game board to represent the state:\n\n- **Player and Opponent Piece Positions:** Binary features indicating the presence of the agent's or opponent's pieces in each cell.\n- **Center Column Control:** A feature representing the proportion of the center column occupied by the agent's pieces.\n- **Threat Potential:** Features counting the number of potential winning lines (4-in-a-row) for the agent and the opponent that are currently unfilled but contain at least one piece of that player.",
      "metadata": {
        "id": "J10ZGQbpNs5f"
      }
    },
    {
      "id": "JldAGWTWLpO-",
      "cell_type": "markdown",
      "source": "## Parameters\n\n- **alpha ($\\alpha$):** The learning rate (0.03). This parameter determines how much the weights are adjusted with each update. A smaller value leads to slower but potentially more stable learning.\n- **gamma ($\\gamma$):** The discount factor (0.9). This parameter determines the importance of future rewards. A value closer to 1 means the agent considers future rewards more heavily.\n- **lambda ($\\lambda$):** The eligibility trace decay rate (0.8). This parameter controls how quickly the eligibility traces decay. A higher value means that credit is given to more distant past states.\n- **epsilon ($\\epsilon$):** The exploration rate (starts at 1.0 and decays to 0.05). This parameter determines the probability of choosing a random action during training. It is crucial for exploring the state space and finding optimal strategies. The decay schedule ensures that the agent exploits its learned knowledge more as training progresses.\n- **Minimax Depth:** The `minimax_move` function uses a depth of 3 for evaluating moves when playing against the minimax agent during training or evaluation. This determines how many steps ahead the minimax algorithm looks.\n- **Training Episodes:** The number of episodes the agent trains for (100,000 in the commented-out training code). More episodes generally lead to better performance, but also require more computational time.\n- **Evaluation Games:** The number of games played against the minimax agent during evaluation (100). This helps to get a more reliable estimate of the agent's win rate.\n\nThese parameters were chosen based on common values used in reinforcement learning for similar problems, then fine-tuned to where they are now. The epsilon decay schedule is designed to allow for sufficient exploration early in training and transition to exploitation later.",
      "metadata": {
        "id": "JldAGWTWLpO-"
      }
    },
    {
      "id": "a05eed98-fa7a-451b-bdd1-3b13aedbfb17",
      "cell_type": "code",
      "source": "#Training Functions\ndef enhanced_features(board, player):\n    opponent = PLAYER1 if player == PLAYER2 else PLAYER2\n    features = []\n\n    #Player and opponent piece positions (binary encodings)\n    player_board = (board == player).astype(int)\n    opp_board = (board == opponent).astype(int)\n    features.extend(player_board.flatten())\n    features.extend(opp_board.flatten())\n\n    #Center column feature\n    center_col = COLUMN_COUNT // 2\n    center_array = board[:, center_col]\n    center_control = np.sum(center_array == player) / ROW_COUNT\n    features.append(center_control)\n\n    #Threat potential features\n    player_threats = count_threats(board, player)\n    opponent_threats = count_threats(board, opponent)\n    features.append(player_threats / 50.0)\n    features.append(opponent_threats / 50.0)\n\n    return np.array(features)\n\n\ndef count_threats(board, piece):\n    count = 0\n\n    #Check all possible 4-in-a-row slices\n    for r in range(ROW_COUNT):\n        for c in range(COLUMN_COUNT - 3):\n            window = board[r, c:c+4]\n            if is_threat_window(window, piece):\n                count += 1\n\n    for r in range(ROW_COUNT - 3):\n        for c in range(COLUMN_COUNT):\n            window = board[r:r+4, c]\n            if is_threat_window(window, piece):\n                count += 1\n\n    for r in range(ROW_COUNT - 3):\n        for c in range(COLUMN_COUNT - 3):\n            window = [board[r+i][c+i] for i in range(4)]\n            if is_threat_window(window, piece):\n                count += 1\n\n    for r in range(ROW_COUNT - 3):\n        for c in range(COLUMN_COUNT - 3):\n            window = [board[r+3-i][c+i] for i in range(4)]\n            if is_threat_window(window, piece):\n                count += 1\n\n    return count\n\ndef is_threat_window(window, piece):\n    return (np.count_nonzero(window == piece) > 0 and\n            np.count_nonzero(window == EMPTY) == 4 - np.count_nonzero(window == piece))\n\ndef creates_double_sided_two(board, row, col, piece):\n    directions = [(0, 1), (1, 0), (1, 1), (1, -1)]  #horiz, vert, diag1, diag2\n    for dr, dc in directions:\n        count = 1\n        empty_left = False\n        empty_right = False\n\n        #Check one direction\n        r, c = row + dr, col + dc\n        while 0 <= r < ROW_COUNT and 0 <= c < COLUMN_COUNT and board[r][c] == piece:\n            count += 1\n            r += dr\n            c += dc\n        if 0 <= r < ROW_COUNT and 0 <= c < COLUMN_COUNT and board[r][c] == EMPTY:\n            empty_right = True\n\n        #Check opposite direction\n        r, c = row - dr, col - dc\n        while 0 <= r < ROW_COUNT and 0 <= c < COLUMN_COUNT and board[r][c] == piece:\n            count += 1\n            r -= dr\n            c -= dc\n        if 0 <= r < ROW_COUNT and 0 <= c < COLUMN_COUNT and board[r][c] == EMPTY:\n            empty_left = True\n\n        if count == 2 and empty_left and empty_right:\n            return True\n\n    return False\n\ndef train_td_lambda(agent1, agent2, env, episodes=100000):\n    win_history = []\n    best_winrate = 0\n    decay_episodes = episodes // 20  #reach 0.05 after 1/20th of episodes\n\n    for ep in range(episodes):\n        #Epsilon decay\n        epsilon = max(0.05, 1.0 - (0.95 * ep / decay_episodes))\n        agent1.set_epsilon(epsilon)\n        agent2.set_epsilon(epsilon)\n\n        state = env.reset()\n        done = False\n        state_prev = None\n\n        #Alternate between self-play and minimax every other episode\n        use_minimax = (ep % 2 == 0)\n        current_agent = agent1\n        other_agent = agent2\n        current_player = agent1.player\n\n        while not done:\n            if use_minimax and current_agent == agent2:\n                action, _ = minimax_move(env, player=current_player)\n                if action is None:\n                    action = random.choice(env.valid_actions())\n            else:\n                action = current_agent.choose_action(env)\n\n            next_state, reward, done = env.step(action, current_player)\n\n            if done:\n                current_agent.update(state, reward, next_state, done)\n                if state_prev is not None:\n                    other_agent.update(state_prev, -reward, next_state, done)\n                win_history.append(env.winner)\n                break\n\n            if state_prev is not None:\n                current_agent.update(state_prev, 0, state, False)\n\n            state_prev = state\n            state = next_state\n\n            current_agent, other_agent = other_agent, current_agent\n            current_player = PLAYER1 if current_player == PLAYER2 else PLAYER2\n\n        #Evaluation and checkpoint\n        if (ep + 1) % 500 == 0:\n            winrate = evaluate_vs_minimax(agent1, env)\n            print(f\"Episode {ep+1}, Epsilon: {epsilon:.4f}, Winrate: {winrate:.2f}\")\n            if winrate > best_winrate:\n                best_winrate = winrate\n                agent1.save_weights(\"best_agent2.npy\")\n\n    return win_history\n\ndef minimax_move(env, depth=3, maximizing=True, player=PLAYER2):\n    opponent = PLAYER1 if player == PLAYER2 else PLAYER2\n    valid_moves = env.valid_actions()\n\n    def evaluate(board):\n        return np.sum(board == player) - np.sum(board == opponent)\n\n    if depth == 0 or env.done:\n        return None, evaluate(env.board)\n\n    best_move = None\n    if maximizing:\n        max_eval = -float('inf')\n        for move in valid_moves:\n            temp_env = Connect4Env()\n            temp_env.board = env.board.copy()\n            temp_env.drop_piece(move, player)\n            temp_env.done = temp_env.winning_move(player)\n            _, eval_val = minimax_move(temp_env, depth-1, False, player)\n            if eval_val > max_eval:\n                max_eval = eval_val\n                best_move = move\n        return best_move, max_eval\n    else:\n        min_eval = float('inf')\n        for move in valid_moves:\n            temp_env = Connect4Env()\n            temp_env.board = env.board.copy()\n            temp_env.drop_piece(move, opponent)\n            temp_env.done = temp_env.winning_move(opponent)\n            _, eval_val = minimax_move(temp_env, depth-1, True, player)\n            if eval_val < min_eval:\n                min_eval = eval_val\n                best_move = move\n        return best_move, min_eval\n\ndef evaluate_vs_minimax(agent, env, games=100):\n    wins = 0\n    draws = 0\n    for _ in range(games):\n        state = env.reset()\n        done = False\n        player = PLAYER1\n        current_agent = agent\n\n        while not done:\n            if player == agent.player:\n                action = agent.choose_action(env)\n            else:\n                action, _ = minimax_move(env, player)\n                if action is None:\n                    action = random.choice(env.valid_actions())\n\n            state, reward, done = env.step(action, player)\n            player = PLAYER1 if player == PLAYER2 else PLAYER2\n\n        if env.winner == agent.player:\n            wins += 1\n        elif env.winner == 0:\n            draws += 1\n\n    winrate = wins / games\n    return winrate",
      "metadata": {
        "id": "a05eed98-fa7a-451b-bdd1-3b13aedbfb17",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "-NYTnWbkOdd1",
      "cell_type": "markdown",
      "source": "Now, we will train the agent using weights from a previous run-through as the starting point.",
      "metadata": {
        "id": "-NYTnWbkOdd1"
      }
    },
    {
      "id": "b9939f5a-e028-4871-b74a-5be3e7c4278f",
      "cell_type": "code",
      "source": "#Train the Agent\nif __name__ == \"__main__\":\n    env = Connect4Env()\n    agent1 = TDLambdaAgent(PLAYER2)\n    agent2 = TDLambdaAgent(PLAYER1)\n\n    #Load previous weights\n    agent1.load_weights(\"best_agent1.npy\")\n    print(\"Loaded saved weights.\")\n\n    print(\"Training agents with TD(lambda) learning and self-play...\")\n    train_td_lambda(agent1, agent2, env, episodes=10000)",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9939f5a-e028-4871-b74a-5be3e7c4278f",
        "outputId": "cc103a89-8a03-41fb-9787-9ebc0c45ae6d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Loaded saved weights.\nTraining agents with TD(lambda) learning and self-play...\n"
        }
      ],
      "execution_count": 7
    },
    {
      "id": "yZNMiUIWOpJU",
      "cell_type": "markdown",
      "source": "This code allows us to play against the agent using a PyGame GUI.",
      "metadata": {
        "id": "yZNMiUIWOpJU"
      }
    },
    {
      "id": "f5404ab9-3540-4dcf-9fe2-e6f090405275",
      "cell_type": "code",
      "source": "#Play against the Agent\nagent1 = TDLambdaAgent(PLAYER2)\nagent1.load_weights(\"best_agent2.npy\")\n\nplay_gui(agent1, env)",
      "metadata": {
        "id": "f5404ab9-3540-4dcf-9fe2-e6f090405275",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "id": "352299f3-e3a2-4129-bb2e-a1182f36a8e2",
      "cell_type": "markdown",
      "source": "## Conclusion",
      "metadata": {}
    },
    {
      "id": "e924e8a2-a0aa-4c5d-91fa-c318dfdc7ddb",
      "cell_type": "markdown",
      "source": "I am pretty happy with the result of this project. I would have preferred to have it train for much longer (100,000 episodes rather than 10,000), but with the help of some functions to reduce the training time, it performs somewhat well. Some of the moves are not ideal and seem random but it can still beat me almost as many times as I can beat it. Next time, I will try to have it train longer and maybe make it unique by having the board rotate 90 degrees every so often.",
      "metadata": {}
    }
  ]
}